---
sidebar_position: 3
title: "üöÄ Getting Started"
---

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';
import { TopBanners } from "@site/src/components/TopBanners";

<TopBanners />

## How to Install üöÄ

:::info **Important Note on User Roles and Privacy:**

- **Admin Creation:** The first account created on Open WebUI gains **Administrator privileges**, controlling user management and system settings.
- **User Registrations:** Subsequent sign-ups start with **Pending** status, requiring Administrator approval for access.
- **Privacy and Data Security:** **All your data**, including login details, is **locally stored** on your device. Open WebUI ensures **strict confidentiality** and **no external requests** for enhanced privacy and security.

:::

## Quick Start with Docker üê≥ (Recommended)

:::tip

### Disabling Login for Single User

If you want to disable login for a single-user setup, set the environment variable [`WEBUI_AUTH`](./env-configuration) to `False`. This will bypass the login page.

:::warning
You **cannot switch back** from single-user mode to multi-account mode after making this change.
:::

:::danger **Important: Data Persistence**

When using Docker, ensure the **database is properly mounted** by including:

```bash
-v open-webui:/app/backend/data
```

Skipping this step may result in **data loss**.
:::

## Installation Instructions

You can install Open WebUI using Docker, Python (pip), or Kubernetes. Each tab below covers a specific installation method.

<Tabs>
<TabItem value="docker-compose" label="Docker Compose">

### Using Docker Compose

If Docker Compose is installed, use this command to start Open WebUI:

```bash
docker compose up -d --build
```

To install with **Nvidia GPU support**:

```bash
docker compose -f docker-compose.yaml -f docker-compose.gpu.yaml up -d --build
```

For **AMD GPUs**, use:

```bash
HSA_OVERRIDE_GFX_VERSION=11.0.0 docker compose -f docker-compose.yaml -f docker-compose.amdgpu.yaml up -d --build
```

**Expose the Ollama API:**

```bash
docker compose -f docker-compose.yaml -f docker-compose.api.yaml up -d --build
```

**Optional: Run with helper script** (for advanced configuration):

```bash
chmod +x run-compose.sh
./run-compose.sh --enable-gpu --build
```

</TabItem>

<TabItem value="docker-manual" label="Docker Manual">

### Manual Docker Installation

If Ollama is already installed, start Open WebUI using:

```bash
docker run -d -p 3000:8080 -v open-webui:/app/backend/data --name open-webui ghcr.io/open-webui/open-webui:main
```

For **Nvidia GPU support**:

```bash
docker run -d -p 3000:8080 --gpus all -v ollama:/root/.ollama -v open-webui:/app/backend/data ghcr.io/open-webui/open-webui:cuda
```

If you want to connect **Ollama on another server**:

```bash
docker run -d -p 3000:8080 -e OLLAMA_BASE_URL=http://<server-ip>:11434 ghcr.io/open-webui/open-webui:main
```

</TabItem>

<TabItem value="python" label="Python (Pip)">

### Install with Python Pip

```bash
pip install open-webui
```

Then, start the server:

```bash
open-webui serve
```

After installation, access Open WebUI at:

[http://localhost:8080](http://localhost:8080)

</TabItem>

<TabItem value="kubernetes" label="Kubernetes (Helm & Kustomize)">

### Deploy with Kubernetes

For **Helm Charts**:

```bash
helm install open-webui ./helm-chart
```

For **Kustomize**:

```bash
kubectl apply -k ./k8s-manifests
```

Ensure your environment is properly configured for Kubernetes, and refer to our documentation for further details.

</TabItem>
</Tabs>

## Data Storage and Bind Mounts

This project uses [Docker named volumes](https://docs.docker.com/storage/volumes/) to **persist data**. If needed, replace the volume name with a host directory:

**Example**:

```bash
-v /path/to/folder:/app/backend/data
```

Ensure the host folder has the correct permissions.
## Manual Setup from GitHub Repo

If you want to build Open WebUI from source, follow these instructions.

```bash
git clone https://github.com/open-webui/open-webui.git
cd open-webui
```

For Linux/macOS:

```bash
cp .env.example .env
npm install
npm run build
cd backend
bash start.sh
```

For Windows:

```powershell
copy .env.example .env
npm install
npm run build
cd backend
start.bat
```

This builds and launches the frontend and backend servers.

## Podman Installation (Rootless)

You can also deploy Open WebUI using **Podman**.

```bash
podman run -d -p 3000:8080 -v open-webui:/app/backend/data ghcr.io/open-webui/open-webui:main
```

If networking issues arise, use:

```bash
--network=slirp4netns:allow_host_loopback=true
```

Refer to the Podman [documentation](https://github.com/containers/podman) for advanced configurations.

## Next Steps

After installing, visit:

- [http://localhost:8080](http://localhost:8080) to verify the frontend.
- Use the **Ollama API**: Check the model list with:

  ```bash
  curl http://localhost:11434/v1/models
  ```

Explore more in our [Next Steps](./next-steps) guide.

## Updating Your Docker Installation

For Docker users, updating Open WebUI can be done manually or via **Watchtower**.

### Manual Update

Stop the container:

```bash
docker stop open-webui
```

Remove the old container:

```bash
docker rm open-webui
```

Pull the latest version:

```bash
docker pull ghcr.io/open-webui/open-webui:main
```

Restart the container:

```bash
docker run -d -p 3000:8080 -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main
```

### Automated Updates with Watchtower

Use **Watchtower** to keep your containers up-to-date automatically.

1. Start Watchtower:

   ```bash
   docker run --rm --volume /var/run/docker.sock:/var/run/docker.sock containrrr/watchtower --run-once open-webui
   ```

2. Replace `open-webui` with your container name if different.

## Troubleshooting Tips

### 1. Verify Ollama Installation

To ensure Ollama is accessible, run:

```bash
curl http://localhost:11434/v1/models
```

If this command returns a list of models, your Ollama instance is working correctly.

### 2. Port Conflicts

- Ensure the **Ollama API** is not blocked by firewalls or conflicting with other services on **port 11434** or **11435**.
- If you are using Docker Compose, confirm the correct mapping with:

  ```yaml
  ports:
    - "11435:11434"
  ```

### 3. GPU Issues on AMD Systems

Some AMD GPUs require:

```bash
HSA_OVERRIDE_GFX_VERSION=10.3.0
```

Apply this environment variable before launching Docker.

## Frequently Asked Questions (FAQ)

### 1. How do I switch from single-user to multi-user mode?

You must reconfigure the `WEBUI_AUTH` environment variable and restart the container. **Be cautious**‚Äîyou cannot switch modes without restarting the application.

### 2. Why can't I access Ollama from another container?

Ensure the following settings:

```bash
--add-host=host.docker.internal:host-gateway
```

This allows the containers to communicate with the host machine.

## Development Setup

For developers who want to contribute, follow the **Development Guide** for a proper setup.

```bash
git clone https://github.com/open-webui/open-webui.git
cd open-webui
npm install
npm run dev
```

The frontend server will be available at [http://localhost:5173](http://localhost:5173), and the backend at [http://localhost:8080](http://localhost:8080).

For Windows, use:

```powershell
npm run dev
start.bat
```

Refer to the full [Development Guide](./development) for detailed steps.

## Using Kubernetes (Kustomize & Helm)

### Helm Chart Installation

```bash
helm install open-webui ./helm-chart
```

### Kustomize Manifests

```bash
kubectl apply -k ./k8s-manifests
```

Ensure your Kubernetes cluster is correctly configured and refer to our [Getting Started](./) guide for troubleshooting.

## Using the WebUI for the First Time

Once installed, access Open WebUI at:

[http://localhost:8080](http://localhost:8080)

Upload your first model using Ollama by selecting **"Upload Model"** from the dashboard. Follow the instructions to load the **Llama 3.2 model**.

## Join the Community

Need help? Have questions? Join our community:

- [Open WebUI Discord](https://discord.gg/5rJgQTnV4s)
- [GitHub Issues](https://github.com/open-webui/open-webui/issues)

Stay updated with the latest features, troubleshooting tips, and announcements!

## Conclusion

Thank you for choosing Open WebUI! We are committed to providing a powerful, privacy-focused interface for your LLM needs. If you encounter any issues, refer to the [Troubleshooting Guide](../troubleshooting/).

Happy exploring! üéâ
